# Introduction {#intro}

## Statistics review

A *random variable* is a variable that can assume various (typically numerical) outcomes subject to a random process. The probabilities of these outcomes are described by a *probability distribution*. Random variables and their distributions are described and defined by parameters, generically denoted as $\theta$ for one parameter or $\vec{\theta} = \theta_1, \dots, \theta_p$ for a collection of parameters. At any time, values of the random variable can be recorded; such recordings are called *observations* or *random variates*. Random variables tend to be denoted with upper-case letters (e.g. $X$) while observations tend to be denoted with lower-case letters (e.g. $x$).

A *random sample* is a collection of $n$ random variables that are identically and independently distributed (iid). In this text, random samples will be denoted with vector notation such as $\vec{X} = X_1, \dots, X_n$. The *observed sample* is the collection of observed values of a random sample, $\vec{x} = x_1, \dots, x_n$. 

A *statistic* is a function of the random sample and is itself a random variable. An *estimator* is a statistic intended to estimate some parameter $\theta$ and may be denoted as $W(\vec{X})=W$. Because an estimator is also a statistic, it is a random variable. An *estimate* is an observation of $W$ and is a numerical outcome of $W(\vec{x})=w$; it is a function of the observed sample. 

Properties of some estimator $W$ of $\theta$ are important for statistical inference. Ideally, $W$ is *accurate* and *precise* or $\mathrm{E}W-\theta$ is small (accuracy) and $\mathrm{Var}W$ is small (precision). The expression $W-\theta$ is referred to as error and its expectation $\mathrm{E}\left(W-\theta\right) = \mathrm{E}W-\theta$ is referred to as *bias*. The expectation of squared errors or *mean squared error* (MSE) is expressed as $\mathrm{E}\left[\left(W-\theta\right)^2\right]$. Mathematical proofs will show that MSE can also be expressed as $\mathrm{Var}W + \mathrm{Bias}^2W$. That is, MSE contains information about the precision and accuracy of $W$. If $W$ is unbiased, $\mathrm{E}W=\theta$, then $\mathrm{MSE}(W)=\mathrm{Var}W$. Ratios of MSEs are often used to compare estimators.

In some cases, we can derive the distribution of $W$, the *sampling distribution*, and use its properties to analytically calculate $E(W)$ and $Var(W)$ and thus and bias and MSE. For example, consider the sample mean of a random sample from the normal distribution. Let  $X_1, \dots, X_n$ be iid normal with some mean $\mu$ and some variance $\sigma^2$. That is, $X_i \sim N(\mu, \sigma^2)$. The sample mean is calculated as $\bar{X} = n^{-1}\sum^n_{i=1}{X_i}$ and is an estimator of $\mu$. It is known that $\bar{X} \sim N(\mu, n^{-1}\sigma^2)$.

$\mathrm{Bias}(\bar{X})=\mathrm{E}\left(\bar{X} - \mu\right) = \mathrm{E}\bar{X} - \mu = \mu-\mu=0$, so $\bar{X}$ is an unbiased estimator of $\mu$. Thus, $\mathrm{MSE}(W) = \mathrm{Var}W = n^{-1}\sigma^2$. The variance of $W$ decreases as $n$ increases; the precision of $\bar{X}$ as an estimator of $\mu$ improves with larger sample sizes. 




## Simulation studies

Analytical approaches such as above are not always possible. The sampling distribution of $W$ may not be available or the necessary mathematical operations are untenable. Suppose, however, that we have a random sample of iid estimators $W_1, \dots, W_m$ and the distribution of $W_i$ is unknown. Furthermore, we have observations of the random sample $w_1, \dots, w_m$. The sample mean $\bar{w}$ of $w_1, \dots, w_m$ is an estimate of $\mathrm{E}W$ and the sample variance $s^2_W$ is an estimate of $\mathrm{Var}W$. As $m$ increases, the quality of the estimates improves.

Each $w_i$ is the result of $W(\mathbf{d}_i)$ where $\mathbf{d}_i$ is an observed sample or dataset. The observed data 







