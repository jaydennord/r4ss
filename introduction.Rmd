# Introduction {#intro}

## Statistics review

A *random variable* is a variable that can assume various (typically numerical) outcomes subject to a random process. The probabilities of these outcomes are described by a *probability distribution*. Random variables and their distributions are described and defined by parameters, generically denoted as $\theta$ for one parameter or $\vec{\theta} = \theta_1, \dots, \theta_p$ for a collection of parameters. At any time, values of the random variable can be recorded; such recordings are called *observations* or *random variates*. Random variables tend to be denoted with upper-case letters (e.g. $X$) while observations tend to be denoted with lower-case letters (e.g. $x$).

A *random sample* is a collection of $n$ random variables that are identically and independently distributed (iid). In this text, random samples will be denoted with vector notation such as $\vec{X} = X_1, \dots, X_n$. The *observed sample* is the collection of observed values of a random sample, $\vec{x} = x_1, \dots, x_n$. 

A *statistic* is a function of the random sample and is itself a random variable. An *estimator* is a statistic intended to estimate some parameter $\theta$ and may be denoted as $W(\vec{X})=W$. Because an estimator is also a statistic, it is a random variable. An *estimate* is an observation of $W$ and is a numerical outcome of $W(\vec{x})=w$; it is a function of the observed sample. 

Properties of some estimator $W$ of $\theta$ are important for statistical inference. Ideally, $W$ is *accurate* and *precise* or $\mathrm{E}W-\theta$ is small (accuracy) and $\mathrm{Var}W$ is small (precision). The expression $W-\theta$ is referred to as error and its expectation $\mathrm{E}\left(W-\theta\right) = \mathrm{E}W-\theta$ is referred to as *bias*. The expectation of squared errors or *mean squared error* (MSE) is expressed as $\mathrm{E}\left[\left(W-\theta\right)^2\right]$. Mathematical proofs will show that MSE can also be expressed as $\mathrm{Var}W + \mathrm{Bias}^2W$. That is, MSE contains information about the precision and accuracy of $W$. If $W$ is unbiased, $\mathrm{E}W=\theta$, then $\mathrm{MSE}(W)=\mathrm{Var}W$. Ratios of MSEs are often used to compare estimators.

In some cases, we can derive the distribution of $W$, the *sampling distribution*, and use its properties to analytically calculate $E(W)$ and $Var(W)$ and thus and bias and MSE. For example, consider the sample mean of a random sample from the normal distribution. Let  $X_1, \dots, X_n$ be iid normal with some mean $\mu$ and some variance $\sigma^2$. That is, $X_i \sim N(\mu, \sigma^2)$. The sample mean is calculated as $\bar{X} = n^{-1}\sum^n_{i=1}{X_i}$ and is an estimator of $\mu$. It is known that $\bar{X} \sim N(\mu, n^{-1}\sigma^2)$.

$\mathrm{Bias}(\bar{X})=\mathrm{E}\left(\bar{X} - \mu\right) = \mathrm{E}\bar{X} - \mu = \mu-\mu=0$, so $\bar{X}$ is an unbiased estimator of $\mu$. Thus, $\mathrm{MSE}(W) = \mathrm{Var}W = n^{-1}\sigma^2$. The variance of $W$ decreases as $n$ increases; the precision of $\bar{X}$ as an estimator of $\mu$ improves with larger sample sizes. 




## Simulation studies

Analytical approaches such as above are not always possible. The sampling distribution of $W$ may not be available or the necessary mathematical operations are untenable. Suppose, however, that we have a random sample of iid estimators $W_1, \dots, W_m$, the distribution of $W_i$ is unknown, and $m$ is the total numbre of *replications* while $i$ is an index for a replication. Furthermore, we have observations of the random sample $w_1, \dots, w_m$. The sample mean $\bar{w}$ of $w_1, \dots, w_m$ is an estimate of $\mathrm{E}W$ and the sample variance $s^2_W$ is an estimate of $\mathrm{Var}W$. As $m$ increases, the quality of the estimates improves.

Each $w_i$ is the result of $W(\mathbf{d}_i)$ where $\mathbf{d}_i$ is an observed sample or dataset. The observed data could be a single variable or a collection of variables presumed to follow a model and distribution with some parameters  $\vec{\theta}$. The parameters could be distribution parameters (mean, variance), regression coefficients, or some other statistic that defines the model. 

In applied statistics, we are interested in estimating and making inferences about some parameter $\theta$. Usually, only one replication of $n$ cases is gathered from the population and the analyst supposes a model to estimate $\theta$. It is assumed that the observed data are the realized values of random samples from a defined distribution or model. The analyst then makes inferences about the population using estimates of $\theta$. 

In simulation studies, we are interested in the properties of some $W$ that is an estimator of $\theta$. 




## R


